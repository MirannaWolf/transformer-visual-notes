# Конспект статьи "The Illustrated Transformer"

Этот документ — мой персональный конспект, основанный на статье Джея Аламмара "The Illustrated Transformer". Цель — переформулировать ключевые концепции на свой язык для лучшего понимания.

## 1. Что такое Attention и зачем он нужен

Представьте, что вы переводите предложение: "The black cat sat on the mat". Когда вы переводите слово "sat" (сидел), ваш мозг интуитивно фокусируется на словах "cat" (кто сидел?) и "mat" (на чем сидел?), а не на слове "The".

Механизм внимания (Attention) делает то же самое для нейросети. Он позволяет модели при обработке одного элемента последовательности (например, слова) взвешивать важность всех остальных элементов и концентрироваться только на самых релевантных. Это решает проблему "бутылочного горлышка" в старых моделях (RNN), где вся информация о предложении сжималась в один вектор состояния, теряя по пути важные детали.

### Формула
В основе внимания лежат три вектора, которые модель сама обучается создавать для каждого слова:

- **Query (Q)**: "Запрос". Это текущее слово, для которого мы хотим вычислить "внимание".
- **Key (K)**: "Ключ". Это "метка" каждого слова в предложении, с которой сравнивается Query.
- **Value (V)**: "Значение". Это фактическое содержание слова, которое мы хотим использовать.

Процесс выглядит так:
1. Сходство между "Запросом" (Q) текущего слова и "Ключом" (K) каждого другого слова вычисляется через скалярное произведение.
2. Результаты масштабируются (делятся на `sqrt(d_k)` для стабильности градиентов).
3. Применяется функция `softmax`, чтобы превратить оценки в вероятности (веса внимания).
4. Эти веса умножаются на "Значения" (V) каждого слова, чтобы "усилить" релевантные слова и "ослабить" нерелевантные.
   
Attention(Q, K, V) = softmax((Q * K^T) / sqrt(d_k)) * V
- **Q** — матрица запросов.
- **K^T** — транспонированная матрица ключей.
- **V** — матрица значений.
- **d_k** — размерность векторов ключей и запросов.

### Иллюстрация
`transformer_self-attention_visualization_2.png`

---
## 2. Как работает Encoder и Decoder

Архитектура Трансформера состоит из двух основных частей: Энкодера (Encoder) и Декодера (Decoder).

### Энкодер
- **Задача**: "Прочитать" и "понять" входное предложение.
- **Структура**: Состоит из стопки одинаковых слоев.
- **Процесс**: Каждый слой принимает на вход эмбеддинги слов (векторные представления) и пропускает их через механизм self-attention (внимание к самому себе), а затем через небольшую нейронную сеть (Feed-Forward Network).
- **Выход**: Обогащенное представление каждого слова, которое учитывает контекст всего предложения.

### Декодер
- **Задача**: Сгенерировать выходное предложение (например, перевод).
- **Структура**: Тоже состоит из стопки слоев.
- **Процесс**: Работает пошагово, генерируя по одному слову за раз. На каждом шаге использует:
  - Слова, которые он уже сгенерировал (через masked self-attention).
  - Выходные представления от Энкодера (через cross-attention). Это самый важный шаг: здесь декодер "смотрит" на исходное предложение и решает, на каком слове оттуда сфокусироваться для генерации следующего слова.
- **Аналогия**: Похоже на работу переводчика-человека: сначала он полностью читает и осмысляет предложение на одном языке (работа энкодера), а затем начинает его переводить, слово за словом, постоянно сверяясь с оригиналом (работа декодера).

### Формула
Структурно это можно описать так:

EncodedFeatures = EncoderStack(InputSequence)
OutputSequence = DecoderStack(EncodedFeatures, TargetSequence)

### Иллюстрация
`The_transformer_encoders_decoders.png`

---

## 3. Почему Positional Encoding критичен

Механизм self-attention сам по себе не учитывает порядок слов. Для него предложения "Король победил королеву" и "Королева победила короля" выглядят одинаково, так как он видит только набор слов, а не их последовательность.

Чтобы решить эту проблему, мы добавляем к исходному эмбеддингу каждого слова специальный вектор — позиционное кодирование (Positional Encoding). Этот вектор уникален для каждой позиции в предложении. Он работает как "метка" с координатой или временным штампом, которая сообщает модели, где именно находится слово (первое, второе, десятое и т.д.).

### Формула
В оригинальной статье используются синусоидальные и косинусоидальные функции разной частоты:

PE(pos, 2i) = sin(pos / (10000^(2i/d_model)))
PE(pos, 2i+1) = cos(pos / (10000^(2i/d_model)))
- **pos** — позиция слова в предложении.
- **i** — индекс измерения в векторе эмбеддинга.
- **d_model** — общая размерность эмбеддинга.

Такой подход позволяет модели легко вычислять относительные позиции слов, что крайне важно для понимания языка.

### Иллюстрация
`transformer_positional_encoding_vectors.png`

---

## 4. Multi-Head Attention и Residual Connections

### Multi-Head Attention (Многоголовое внимание)
- Вместо того чтобы вычислять внимание один раз, мы делаем это несколько раз параллельно в разных "проекциях". Каждая такая проекция называется "головой" (head).
- **Аналогия**: Как если бы вы попросили нескольких экспертов прочитать предложение. Один мог бы сфокусироваться на грамматических связях (подлежащее-сказуемое), другой — на логических (причина-следствие), третий — на семантических синонимах. Каждая "голова" учится обращать внимание на разные аспекты взаимосвязей между словами.
- **Процесс**: Результаты всех голов объединяются, чтобы получить более богатое и многогранное представление.

### Формула
MultiHead(Q, K, V) = Concat(head_1, ..., head_h) * W^O
где head_i = Attention(Q * W_i^Q, K * W_i^K, V * W_i^V)
- **W_i^Q, W_i^K, W_i^V** — матрицы весов для каждой "головы".
- **W^O** — выходная матрица весов.

### Residual Connections (Остаточные соединения)
- Вокруг каждого подслоя (например, attention или нейронной сети) есть "обходной путь". Входные данные этого подслоя `x` добавляются к его выходным данным `Sublayer(x)`.
- **Зачем**: Это помогает бороться с проблемой затухания градиентов при обучении очень глубоких сетей. Модель по умолчанию передает информацию дальше без изменений, а подслой лишь добавляет к ней полезные уточнения.
- **Процесс**: После сложения применяется нормализация `LayerNorm`.

### Формула
Output = LayerNorm(x + Sublayer(x))

### Иллюстрация
`transformer_resideual_layer_norm_3`

---

## 5. Как это применяется в GPT

Модели семейства GPT (Generative Pre-trained Transformer) являются генераторами текста. Их основная задача — предсказать следующее слово на основе предыдущих. Им не нужно переводить одну последовательность в другую, поэтому им не нужен Энкодер.

- **Особенность**: GPT использует только архитектуру Декодера из Трансформера.
- **Ключевое отличие**: Self-attention внутри GPT является маскированным (masked self-attention). Это означает, что при вычислении внимания для текущего слова модель может смотреть только на предыдущие слова в последовательности и на само текущее слово. Ей "запрещено" подглядывать в будущее.
- **Логика**: Чтобы предсказать 5-е слово, вы должны знать только первые 4, а не 6-е или 7-е. Эта маска гарантирует, что модель учится генерировать текст последовательно, как это делает человек.

### Иллюстрация
`gpt_decoder`
